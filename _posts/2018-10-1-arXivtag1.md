---
layout: post
title: Building the arXiv classifier - I
---
## Part I: Getting the dataset
### The arXiv dataset

The [arXiv](http://arxiv.org/) is a online repository of preprints of scientific papers in the fields of astronomy, physics, mathematics, computer science, quantitative biology, quantitative finance and statistics. To date it has more than a million papers and more are being added every day. This dataset I focused on is a relatively recent (2007-17) sample totaling approximately 800,000 pieces of metadata which I curated via a data dump using the arXiv APIs. They contain a significant number of papers (>5000) from every category (~10) submitted in the past decade. 

#### <details><summary>Bulk access of arXiv metadata</summary>
##### For harvesting arXiv data year by year
(Please read [here](https://arxiv.org/help/bulk_data) and [here](https://arxiv.org/help/oa/index))
([This](https://academia.stackexchange.com/questions/38969/getting-a-dump-of-arxiv-metadata) SO thread helps a lot too)

**<span style="color:#f78c6c">Please do not DDoS the arXiv server, I accept no responsibility if you get into trouble doing this</span>**

arXiv supports bulk access of their article metadata (updated daily) as well as real-time programmatic access to metadata via the [arXiv API](https://arxiv.org/help/api/index).

A sample http query looks like this:

```html
http://export.arxiv.org/oai2?verb=ListIdentifiers&set=math&metadataPrefix=oai_dc&from=2007-05-23&until=2015-05-24
```
here we have `set=math` and `from=2007-05-23&until=2007-05-24`

A sample response looks like this:

```xml
<Records xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
  <Record>
    <header status="">
      <identifier>oai:arXiv.org:0704.0004</identifier>
      <datestamp>2007-05-23</datestamp>
      <setSpec>math</setSpec>
    </header>
    <metadata>
      <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
        <id>0704.0004</id>
        <created>2007-03-30</created>
        <authors>
          <author>
            <keyname>Callan</keyname>
            <forenames>David</forenames>
          </author>
        </authors>
        <title>A determinant of Stirling cycle numbers counts unlabeled acyclic
         single-source automata</title>
        <categories>math.CO</categories>
        <comments>11 pages</comments>
        <msc-class>05A15</msc-class>
        <abstract>We show that a determinant of Stirling cycle numbers counts unlabeled acyclic single-source automata. The proof involves a bijection from these automata to certain marked lattice paths and a sign-reversing involution to evaluate the determinant.
         </abstract>
      </arXiv>
    </metadata>
    <about/>
  </Record>
</Records>
```
every response has a list of `<Record>` under a `<Records>` tag.

However, if you query more than 1000 articles at once you will get a `resumptionToken` and effectively the server is going to rate limit you. To get around that I wrote a script to wait for 20-30 seconds before issuing the http query again with the resumption token. Something like this

```python
# harvests 1 year worth of arXiv articles
def harvest_by_year(year):
    save_path = "../Data/raw"
    filename = "arXiv" + str(year) + ".xml"
    filename = os.path.join(save_path, filename)
    f = io.open(filename, 'a', encoding="utf-8")
    first_url = "http://export.arxiv.org/oai2?verb=ListRecords&from=" + \
        str(year) + "-01-01&until=" + \
        str(year) + "-12-31&metadataPrefix=arXiv"
    data = urllib.request.urlopen(first_url).read()
    soup = BeautifulSoup(data, 'lxml')
    f.write(soup.prettify())

    token = soup.find('resumptiontoken').text
    resume = True

    # loop over resumption tokens till the end
    while resume:
        # wait for server
        time.sleep(21)
        url = 'http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=' + token

        next_data = urllib.request.urlopen(url).read()
        soup = BeautifulSoup(next_data, 'html.parser')
        f.write(soup.prettify())
        if soup.find('resumptiontoken') is not None:
            token = soup.find('resumptiontoken').text
            if token is "":
                resume = False
                break
        else:
            resume = False
            break
    return
```
and I used [`BeautifulSoup`](https://pypi.org/project/beautifulsoup4/) to clean it up, join the XML and remove the `resumptionToken` in the XML responses. Note that the joined XML files can get very large (big data woooo) even for one year worth.

##### Alternative: bulk download

If small scale tests - and I fully encourage you to do small small scale tests - work, then you can go [here](https://archive.org/details/arxiv-bulk) and download full data sets of arXiv article as well as metadata.
</details>

#### <details><summary>Wrangling text (yes that's the technical term) to get what we need</summary>

Use the `strainer` from `BeautifulSoup` to parse out the `identifier`, `abstract` and `categories` tags and zip them into a tuple list and dump it via [`pickle`].(https://docs.python.org/2/library/pickle.html)

`identifier` is a string like this `oai:arXiv.org:0704.0004` it suffices to take only the rear chunk `0704.0004`.

`categories` is a list of one or more strings like this `math.CO` we are taking the first category for the purposes of this project, the natural extension would be to take the first n categories and do a multi-class, multi-label classifier.

use `utf-8` encoding for text because of the propensity of mathematical symbols in these scientific papers.

```python
    filename = "../Data/raw/arXivbulk.xml"

    strainer_id = SoupStrainer("identifier")
    soup_id = BeautifulSoup(io.open(filename, encoding="utf-8"), "xml", parse_only=strainer_id)

    # truncate just to get id
    id_list = [x[14:] for x in soup_id.strings]

    strainer_abs = SoupStrainer("abstract")
    soup_abs = BeautifulSoup(io.open(filename, encoding="utf-8"), "xml", parse_only=strainer_abs)

    # clean newline and whitespace from abs
    abs_list = [" ".join(x.text.replace('\n', ' ').strip().split()) for x in soup_abs.find_all('abstract')]

    # reduce categories to the first big category in the first word
    strainer_cat = SoupStrainer("categories")
    soup_set = BeautifulSoup(io.open(filename, encoding="utf-8"), "xml", parse_only=strainer_cat)
    set_list = [x.split(' ', 1)[0].split('.', 1)[0] for x in soup_set.strings]
    # print(set_list)

    # build a dictionary with key = id, value = tuple of other things
    keys = id_list
    values = list(zip(set_list, abs_list))
    print(values.__len__())
    article_dic = dict(set(zip(keys, values)))
    print(article_dic.keys().__len__())

    dictname = "../Data/dict/full_articleset.p"
    pickle.dump(article_dic, open(dictname, "wb"))
```
</details>